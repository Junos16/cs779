# CS779 Assignment-2

This assignment focuses on implementing and evaluating subword tokenization algorithms, which are crucial for modern NLP tasks.

The file structure is as follows:
- `input_data.txt`: Contains the encrypted text to be used for building the tokenizer. Your code to train the tokenizer will then be evaluated on our private data to check the quality of tokens. You are free to divide the data in any way to create train and validation sets.
- `<rollno>_assignment2_*.py`: Boilerplate code for training each tokenizer, saving the vocab, tokenizing and detokenizing a sample test data
- `vocab.txt`: The vocabulary file generated by your tokenizer, which will be used for tokenization and detokenization during evaluation.
- `test_input.txt`: This file is not shared but will contain a sample text which has to be tokenized and detokenized using the code to create the `tokens.txt` and `detokenized.txt` files. We will compare the `test_input.txt` file with the `detokenized.txt` file to evaluate the performance of your tokenizer.
- `tokens.txt`: The file containing the tokenized output for the sample test data.
- `detokenized.txt`: The file containing the detokenized output for the sample test data.



#### 1. `vocab-sample.txt`

* Shows the expected structure of your generated vocabulary file.
* The first **four entries** are reserved tokens (e.g., `<pad>`, `<unk>`, `<s>`, `</s>`, ).
* Remaining tokens are listed **in the order they are learned by your tokenizer**.
* Your actual vocab file must follow this exact format:

  * **One token per line**
  * **No extra spaces or separators**

#### 2. `token-sample.txt`

* Demonstrates the format of the tokenized output generated from `sample-text.txt`.
* **Each token is written on a separate line.**
* The special marker `▁` represents whitespace in SentencePiece-style tokenization.
* Tokens include both **Hindi** and **English** text in a single vocabulary.
* This is only an **example format** — your tokens will differ depending on your trained merges.